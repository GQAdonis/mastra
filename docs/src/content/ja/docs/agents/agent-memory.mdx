---
title: "エージェントメモリの使用 | エージェント | Mastra ドキュメント"
description: Mastraのエージェントが会話履歴とコンテキスト情報を保存するためにメモリを使用する方法に関するドキュメント。
---

# エージェントメモリ

Mastraのエージェントは、会話の履歴とコンテキスト情報を保存する高度なメモリシステムを持っています。このメモリシステムは、従来のメッセージストレージとベクトルベースのセマンティック検索の両方をサポートしており、エージェントがインタラクションを通じて状態を維持し、関連する過去のコンテキストを取得することを可能にします。

## スレッドとリソース

Mastraでは、会話を`thread_id`で整理することができます。これにより、システムはコンテキストを維持し、同じディスカッションに属する過去のメッセージを取得することができます。

Mastraはまた、`resource_id`の概念もサポートしています。これは通常、会話に関わるユーザーを表し、エージェントのメモリとコンテキストが正しいエンティティに関連付けられるようにします。

この分離により、単一のユーザーに対して複数の会話（スレッド）を管理したり、必要に応じて会話のコンテキストをユーザー間で共有したりすることができます。

```typescript copy showLineNumbers
import { Agent } from "@mastra/core/agent";
import { openai } from "@ai-sdk/openai";

const agent = new Agent({
  name: "Project Manager",
  instructions:
    "You are a project manager. You are responsible for managing the project and the team.",
  model: openai("gpt-4o-mini"),
});

await agent.stream("When will the project be completed?", {
  threadId: "project_123",
  resourceId: "user_123",
});
```

## 会話コンテキストの管理

LLMから良い応答を得るための鍵は、適切なコンテキストを与えることです。

Mastraには会話履歴とコンテキスト情報を保存・管理するMemory APIがあります。Memory APIはストレージバックエンドを使用して会話履歴とコンテキスト情報を永続化します（詳細は後述）。

Memory APIは会話のコンテキストを維持するために、最近のメッセージ履歴とセマンティック検索という2つの主要なメカニズムを使用します。

### 最近のメッセージ履歴

デフォルトでは、Memoryは会話の中で最新の40メッセージを追跡します。これは`lastMessages`設定でカスタマイズできます：

```typescript copy showLineNumbers
const memory = new Memory({
  options: {
    lastMessages: 5, // 最新の5メッセージを保持
  },
});

// ユーザーがこの質問をすると、エージェントは最新の10メッセージを参照します
await agent.stream("検索機能の要件を要約してもらえますか？", {
  memoryOptions: {
    lastMessages: 10,
  },
});
```

### セマンティック検索

セマンティック検索はMastraではデフォルトで有効になっています。FastEmbed（bge-small-en-v1.5）とLibSQLがデフォルトで含まれていますが、必要に応じて任意のエンベッダー（OpenAIやCohereなど）やベクトルデータベース（PostgreSQL、Pinecone、Chromaなど）を使用できます。

これにより、エージェントは会話の初期段階から関連情報を見つけて思い出すことができます：

```typescript copy showLineNumbers
const memory = new Memory({
  options: {
    semanticRecall: {
      topK: 10, // 最も関連性の高い過去のメッセージを10件含める
      messageRange: 2, // 各結果の前後のメッセージ
    },
  },
});

// 例：ユーザーが過去の機能についての議論について質問
await agent.stream("先週、検索機能について何を決めましたか？", {
  memoryOptions: {
    lastMessages: 10,
    semanticRecall: {
      topK: 3,
      messageRange: 2,
    },
  },
});
```

セマンティック検索が使用される場合：

1. メッセージはベクトル埋め込みに変換されます
2. ベクトル類似性検索を使用して類似したメッセージが見つかります
3. `messageRange`に基づいて周囲のコンテキストが含まれます
4. すべての関連コンテキストがエージェントに提供されます

ベクトルデータベースとエンベッダーもカスタマイズできます：

```typescript copy showLineNumbers
import { openai } from "@ai-sdk/openai";
import { PgVector } from "@mastra/pg";

const memory = new Memory({
  // 異なるベクトルデータベースを使用（デフォルトはlibsql）
  vector: new PgVector("postgresql://user:pass@localhost:5432/db"),
  // または異なるエンベッダー（デフォルトはfastembed）
  embedder: openai.embedding("text-embedding-3-small"),
});
```

## メモリ設定

Mastraのメモリシステムは非常に設定可能で、複数のストレージバックエンドをサポートしています。デフォルトでは、ストレージとベクトル検索にLibSQLを使用し、埋め込みにFastEmbedを使用します。

### 基本設定

ほとんどのユースケースでは、デフォルトの設定を使用できます：

```typescript copy showLineNumbers
import { Memory } from "@mastra/memory";

const memory = new Memory();
```

### カスタム設定

より詳細な制御が必要な場合は、ストレージバックエンド、ベクトルデータベース、およびメモリオプションをカスタマイズできます：

```typescript copy showLineNumbers
import { Memory } from "@mastra/memory";
import { PostgresStore, PgVector } from "@mastra/pg";

const memory = new Memory({
  storage: new PostgresStore({
    host: "localhost",
    port: 5432,
    user: "postgres",
    database: "postgres",
    password: "postgres",
  }),
  vector: new PgVector("postgresql://user:pass@localhost:5432/db"),
  options: {
    // 含める最近のメッセージの数（無効にするにはfalse）
    lastMessages: 10,
    // ベクトルベースのセマンティック検索を設定（無効にするにはfalse）
    semanticRecall: {
      topK: 3, // セマンティック検索結果の数
      messageRange: 2, // 各結果の前後のメッセージ
    },
  },
});
```

### メモリ設定の上書き

メモリ設定でMastraインスタンスを初期化すると、すべてのエージェントは`stream()`または`generate()`メソッドを呼び出す際に自動的にこれらのメモリ設定を使用します。個々の呼び出しに対してこれらのデフォルト設定を上書きすることができます：

```typescript copy showLineNumbers
// Memory設定からデフォルトのメモリ設定を使用
const response1 = await agent.generate("What were we discussing earlier?", {
  resourceId: "user_123",
  threadId: "thread_456",
});

// この特定の呼び出しに対してメモリ設定を上書き
const response2 = await agent.generate("What were we discussing earlier?", {
  resourceId: "user_123",
  threadId: "thread_456",
  memoryOptions: {
    lastMessages: 5, // 最近のメッセージを5つだけ挿入
    semanticRecall: {
      topK: 2, // セマンティック検索結果を2つだけ取得
      messageRange: 1, // 各結果の周囲のコンテキスト
    },
  },
});
```

### 異なるユースケースに対するメモリ設定の構成

エージェントのニーズに基づいてメモリ設定を調整できます：

```typescript copy showLineNumbers
// 最小限のコンテキストを持つカスタマーサポートエージェント
await agent.stream("What are your store hours?", {
  threadId,
  resourceId,
  memoryOptions: {
    lastMessages: 5, // クイックレスポンスには最小限の会話履歴が必要
    semanticRecall: false, // 以前のメッセージを検索する必要はない
  },
});

// 広範なコンテキストを持つプロジェクト管理エージェント
await agent.stream("Update me on the project status", {
  threadId,
  resourceId,
  memoryOptions: {
    lastMessages: 50, // プロジェクトの議論全体で長い会話履歴を維持
    semanticRecall: {
      topK: 5, // より関連性の高いプロジェクトの詳細を見つける
      messageRange: 3, // 各結果の前後のメッセージの数
    },
  },
});
```

## ストレージオプション

Mastraは現在、いくつかのストレージバックエンドをサポートしています：

### LibSQLストレージ

```typescript copy showLineNumbers
import { LibSQLStore } from "@mastra/core/storage/libsql";

const storage = new LibSQLStore({
  config: {
    url: "file:example.db",
  },
});
```

### PostgreSQLストレージ

```typescript copy showLineNumbers
import { PostgresStore } from "@mastra/pg";

const storage = new PostgresStore({
  host: "localhost",
  port: 5432,
  user: "postgres",
  database: "postgres",
  password: "postgres",
});
```

### Upstash KVストレージ

```typescript copy showLineNumbers
import { UpstashStore } from "@mastra/upstash";

const storage = new UpstashStore({
  url: "http://localhost:8089",
  token: "your_token",
});
```

## ベクター検索

Mastraはベクター埋め込みを通じてセマンティック検索をサポートしています。ベクターストアを設定すると、エージェントはセマンティック類似性に基づいて関連する過去のメッセージを見つけることができます。ベクター検索を有効にするには：

1. ベクターストアを設定します（現在はPostgreSQLをサポート）：

```typescript copy showLineNumbers
import { PgVector } from "@mastra/pg";

const vector = new PgVector(connectionString);

const memory = new Memory({ vector });
```

2. 埋め込みオプションを設定します：

```typescript copy showLineNumbers
const memory = new Memory({
  vector,
  embedder: openai.embedding("text-embedding-3-small"),
});
```

3. メモリ設定オプションでベクター検索を有効にします：

```typescript copy showLineNumbers
const memory = new Memory({
  vector,
  embedder,

  options: {
    semanticRecall: {
      topK: 3, // 見つける類似メッセージの数
      messageRange: 2, // 各結果の周囲のコンテキスト
    },
  },
});
```

## エージェントでのメモリの使用

一度設定されると、メモリシステムは自動的にエージェントによって使用されます。使用方法は次のとおりです：

```typescript copy showLineNumbers
// メモリを持つエージェントを初期化
const myAgent = new Agent({
  memory,
  // 他のエージェントオプション
});
// エージェントをmastraに追加
const mastra = new Mastra({
  agents: { myAgent },
});

// resourceIdとthreadIdが追加されると、エージェントの対話でメモリが自動的に使用されます
const response = await myAgent.generate(
  "パフォーマンスについて以前に話していたことは何でしたか？",
  {
    resourceId: "user_123",
    threadId: "thread_456",
  },
);
```

メモリシステムは自動的に以下を行います：

1. すべてのメッセージを設定されたストレージバックエンドに保存
2. セマンティック検索のためのベクトル埋め込みを作成（設定されている場合）
3. 新しい会話に関連する過去のコンテキストを注入
4. 会話スレッドとコンテキストを維持

## useChat()

AI SDKから`useChat`を使用する場合、最新のメッセージのみを送信しないとメッセージの順序に関するバグが発生します。

お使いのフレームワークの`useChat()`実装が`experimental_prepareRequestBody`をサポートしている場合、以下のようにすることができます：

```ts
const { messages } = useChat({
  api: "api/chat",
  experimental_prepareRequestBody({ messages, id }) {
    return { message: messages.at(-1), id };
  },
});
```

これにより、常に最新のメッセージのみがサーバーに送信されます。
チャットサーバーエンドポイントでは、streamまたはgenerateを呼び出す際にthreadIdとresourceIdを渡すことができ、エージェントはメモリスレッドのメッセージにアクセスできるようになります：

```ts
const { messages } = await request.json();

const stream = await myAgent.stream(messages, {
  threadId,
  resourceId,
});

return stream.toDataStreamResponse();
```

お使いのフレームワーク（例えばsvelte）の`useChat()`が`experimental_prepareRequestBody`をサポートしていない場合、streamまたはgenerateを呼び出す前に最後のメッセージを選択して使用することができます：

```ts
const { messages } = await request.json();

const stream = await myAgent.stream([messages.at(-1)], {
  threadId,
  resourceId,
});

return stream.toDataStreamResponse();
```

詳細については、[メッセージ永続化に関するAI SDKのドキュメント](https://sdk.vercel.ai/docs/ai-sdk-ui/chatbot-message-persistence)をご覧ください。

## スレッドの手動管理

スレッドはエージェントメソッドを使用する際に自動的に管理されますが、メモリAPIを直接使用して手動でスレッドを管理することもできます。これは以下のような高度なユースケースに役立ちます：

- 会話を開始する前にスレッドを作成する
- スレッドのメタデータを管理する
- メッセージを明示的に保存または取得する
- 古いスレッドをクリーンアップする

スレッドを手動で操作する方法は次のとおりです：

```typescript copy showLineNumbers
import { Memory } from "@mastra/memory";
import { PostgresStore } from "@mastra/pg";

// Initialize memory
const memory = new Memory({
  storage: new PostgresStore({
    host: "localhost",
    port: 5432,
    user: "postgres",
    database: "postgres",
    password: "postgres",
  }),
});

// Create a new thread
const thread = await memory.createThread({
  resourceId: "user_123",
  title: "Project Discussion",
  metadata: {
    project: "mastra",
    topic: "architecture",
  },
});

// Manually save messages to a thread
await memory.saveMessages({
  messages: [
    {
      id: "msg_1",
      threadId: thread.id,
      role: "user",
      content: "What's the project status?",
      createdAt: new Date(),
      type: "text",
    },
  ],
});

// Get messages from a thread with various filters
const messages = await memory.query({
  threadId: thread.id,
  selectBy: {
    last: 10, // Get last 10 messages
    vectorSearchString: "performance", // Find messages about performance
  },
});

// Get thread by ID
const existingThread = await memory.getThreadById({
  threadId: "thread_123",
});

// Get all threads for a resource
const threads = await memory.getThreadsByResourceId({
  resourceId: "user_123",
});

// Update thread metadata
await memory.updateThread({
  id: thread.id,
  title: "Updated Project Discussion",
  metadata: {
    status: "completed",
  },
});

// Delete a thread and all its messages
await memory.deleteThread(thread.id);
```

ほとんどの場合、エージェントの`generate()`および`stream()`メソッドがスレッド管理を自動的に処理するため、スレッドを手動で管理する必要はありません。手動スレッド管理は主に高度なユースケースや、会話履歴をより細かく制御する必要がある場合に役立ちます。

## ワーキングメモリ

ワーキングメモリは、最小限のコンテキストでも会話全体で永続的な情報を維持できるエージェントの強力な機能です。これは、ユーザーの好み、個人情報、または対話全体で持続すべきその他のコンテキスト情報を記憶するのに特に役立ちます。

MemGPTホワイトペーパーからインスピレーションを得たワーキングメモリのコンセプトを、私たちの実装ではいくつかの重要な点で改善しています：

- 追加のラウンドトリップやツールコールが不要
- メッセージのストリーミングを完全にサポート
- エージェントの自然な応答フローとシームレスに統合

#### 仕組み

ワーキングメモリは、組織化されたデータと自動更新のシステムを通じて機能します：

1. **テンプレート構造**：Markdownを使用して記憶すべき情報を定義します。Memoryクラスには、ユーザー情報のための包括的なデフォルトテンプレートが付属していますが、特定のニーズに合わせて独自のテンプレートを作成することもできます。

2. **自動更新**：Memoryクラスは、エージェントのシステムプロンプトに特別な指示を挿入し、以下のことを指示します：

   - `<working_memory>...</working_memory>`タグを応答に含めることで関連情報を保存する
   - 何か変更があった場合に積極的に情報を更新する
   - 値を更新する際にMarkdown構造を維持する
   - このプロセスをユーザーに見えないようにする

3. **メモリ管理**：システムは：
   - エージェントの応答からワーキングメモリブロックを抽出する
   - 将来の使用のためにそれらを保存する
   - 次のエージェント呼び出し時にワーキングメモリをシステムプロンプトに挿入する

エージェントは情報の保存に積極的であるよう指示されています - 後で役立つかもしれないという疑いがある場合は、保存すべきです。これにより、非常に小さなコンテキストウィンドウを使用している場合でも、会話のコンテキストを維持するのに役立ちます。

#### 基本的な使用法

```typescript copy showLineNumbers
import { openai } from "@ai-sdk/openai";

const agent = new Agent({
  name: "Customer Service",
  instructions:
    "You are a helpful customer service agent. Remember customer preferences and past interactions.",
  model: openai("gpt-4o-mini"),

  memory: new Memory({
    options: {
      workingMemory: {
        enabled: true, // ワーキングメモリを有効にする
      },
      lastMessages: 5, // 最近のコンテキストのみを保持
    },
  }),
});
```

ワーキングメモリは、特殊なシステムプロンプトと組み合わせると特に強力になります。例えば、前のメッセージにしかアクセスできなくても状態を維持するTODOリストマネージャーを作成できます：

```typescript copy showLineNumbers
const todoAgent = new Agent({
  name: "TODO Manager",
  instructions:
    "You are a TODO list manager. Update the todo list in working memory whenever tasks are added, completed, or modified.",
  model: openai("gpt-4o-mini"),
  memory: new Memory({
    options: {
      workingMemory: {
        enabled: true,

        // エージェントが特定の種類の情報を保存するよう促すオプションのMarkdownテンプレート。
        // これを省略するとデフォルトのテンプレートが使用されます
        template: `# Todo List
## In Progress
- 
## Pending
- 
## Completed
- 
`,
      },
      lastMessages: 1, // コンテキストに最後のメッセージのみを保持
    },
  }),
});
```

### ストリーミングでのメモリ更新の処理

エージェントが応答すると、応答ストリームに直接ワーキングメモリの更新が含まれます。これらの更新はテキスト内のタグ付きブロックとして表示されます：

```typescript copy showLineNumbers
// 生のエージェント応答ストリーム：
Let me help you with that! <working_memory># User Information
- **First Name**: John
- **Last Name**:
- **Location**:
...</working_memory> Based on your question...
```

これらのメモリブロックがユーザーに表示されないようにしながら、システムがそれらを処理できるようにするには、`maskStreamTags`ユーティリティを使用します：

```typescript copy showLineNumbers
import { maskStreamTags } from "@mastra/core/utils";

// 基本的な使用法 - working_memoryタグだけをマスクする
for await (const chunk of maskStreamTags(
  response.textStream,
  "working_memory",
)) {
  process.stdout.write(chunk);
}

// マスクなし: "Let me help you! <working_memory>...</working_memory> Based on..."
// マスクあり: "Let me help you! Based on..."
```

メモリ更新イベントにフックすることもできます：

```typescript copy showLineNumbers
const maskedStream = maskStreamTags(response.textStream, "working_memory", {
  onStart: () => showLoadingSpinner(),
  onEnd: () => hideLoadingSpinner(),
  onMask: (chunk) => console.debug(chunk),
});
```

`maskStreamTags`ユーティリティは：

- ストリーミング応答内の指定されたXMLタグ間のコンテンツを削除します
- オプションでメモリ更新のライフサイクルコールバックを提供します
- ストリームチャンク間で分割される可能性のあるタグを処理します

### ツールでのスレッドIDとリソースIDへのアクセス

カスタムツールを作成する際、ツールのexecute関数で直接`threadId`と`resourceId`にアクセスできます。これらのパラメータはMastraランタイムによって自動的に提供されます：

```typescript copy showLineNumbers
import { Memory } from "@mastra/memory";
const memory = new Memory();

const myTool = createTool({
  id: "Thread Info Tool",
  inputSchema: z.object({
    fetchMessages: z.boolean().optional(),
  }),
  description: "A tool that demonstrates accessing thread and resource IDs",
  execute: async ({ threadId, resourceId, context }) => {
    // threadId and resourceId are directly available in the execute parameters
    console.log(`Executing in thread ${threadId}`);

    if (!context.fetchMessages) {
      return { threadId, resourceId };
    }

    const recentMessages = await memory.query({
      threadId,
      selectBy: { last: 5 },
    });

    return {
      threadId,
      resourceId,
      messageCount: recentMessages.length,
    };
  },
});
```

これにより、ツールは以下を行うことができます：

- 現在の会話コンテキストにアクセスする
- スレッド固有のデータを保存または取得する
- 特定のユーザー/リソースとツールアクションを関連付ける
- 複数のツール呼び出しにわたって状態を維持する
