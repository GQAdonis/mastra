---
title: Mastraの音声 | Mastra ドキュメント
description: Mastraにおける音声機能の概要。テキストから音声への変換、音声からテキストへの変換、リアルタイムの音声間のやり取りを含みます。
---

# Mastraの音声

Mastraの音声システムは、音声インタラクションのための統一されたインターフェースを提供し、アプリケーション内でのテキスト読み上げ（TTS）、音声認識（STT）、リアルタイムの音声間通信を可能にします。

## 主な機能

- 異なる音声プロバイダー間での標準化されたAPI
- 複数の音声サービスのサポート
- 継続的な音声ストリーミングのためのイベントを使用した音声間の対話
- TTSとSTTサービスを組み合わせるためのコンポーザブルな音声プロバイダー

## エージェントに音声を追加する

エージェントに音声機能を統合する方法を学ぶには、[エージェントに音声を追加する](../agents/adding-voice.mdx) ドキュメントを参照してください。このセクションでは、単一および複数の音声プロバイダーの使用方法、ならびにリアルタイムの対話について説明します。


## 単一の音声プロバイダーを使用する例

```typescript
import { OpenAIVoice } from "@mastra/voice-openai";

// TTSのためにOpenAI音声を初期化
const voice = new OpenAIVoice({
  speechModel: {
    name: "tts-1-hd", // TTSモデルを指定
    apiKey: process.env.OPENAI_API_KEY, // あなたのOpenAI APIキー
  },
});

// テキストを音声に変換
const audioStream = await voice.speak("こんにちは！今日はどのようにお手伝いできますか？", {
  speaker: "default", // 任意: スピーカーを指定
});

// 音声応答を再生
playAudio(audioStream);
```

## 複数の音声プロバイダーを使用する例
この例では、Mastraで2つの異なる音声プロバイダーを作成して使用する方法を示します。OpenAIは音声認識（STT）用で、PlayAIは音声合成（TTS）用です。

まず、必要な設定を行って音声プロバイダーのインスタンスを作成します。

```typescript
import { OpenAIVoice } from "@mastra/voice-openai";
import { PlayAIVoice } from "@mastra/voice-playai";
import { CompositeVoice } from "@mastra/core/voice";

// STT用にOpenAI音声を初期化
const input = new OpenAIVoice({
  listeningModel: {
    name: "whisper-1",
    apiKey: process.env.OPENAI_API_KEY,
  },
});

// TTS用にPlayAI音声を初期化
const output = new PlayAIVoice({
  speechModel: {
    name: "playai-voice",
    apiKey: process.env.PLAYAI_API_KEY,
  },
});

// CompositeVoiceを使用してプロバイダーを結合
const voice = new CompositeVoice({
  input,
  output,
});

// 結合された音声プロバイダーを使用して音声インタラクションを実装
const audioStream = getMicrophoneStream(); // この関数が音声入力を取得すると仮定
const transcript = await voice.listen(audioStream);

// 書き起こされたテキストをログに記録
console.log("書き起こされたテキスト:", transcript);

// テキストを音声に変換
const responseAudio = await voice.speak(`あなたが言ったのは: ${transcript}`, {
  speaker: "default", // オプション: スピーカーを指定
});

// 音声応答を再生
playAudio(responseAudio);
```

## リアルタイム機能

多くの音声プロバイダーは、WebSocket接続を通じてリアルタイムの音声対音声インタラクションをサポートしており、以下を可能にします：

- AIとのライブ音声会話
- ストリーミング文字起こし
- リアルタイムの音声合成
- 会話中のツール使用


## ボイス設定

ボイスプロバイダーは、異なるモデルやオプションで設定できます:

```typescript
const voice = new OpenAIVoice({
  speechModel: {
    name: "tts-1-hd",
    apiKey: process.env.OPENAI_API_KEY
  },
  listeningModel: {
    name: "whisper-1"
  },
  speaker: "alloy"
});
```

## 利用可能な音声プロバイダー

Mastraは、以下を含むさまざまな音声プロバイダーをサポートしています：

- OpenAI
- PlayAI
- Murf
- ElevenLabs
- [その他](https://github.com/mastra-ai/mastra/tree/main/voice)

## その他のリソース

- [CompositeVoice](../../reference/voice/composite-voice.mdx)
- [MastraVoice](../../reference/voice/mastra-voice.mdx)
- [OpenAI Voice](../../reference/voice/openai.mdx)
- [PlayAI Voice](../../reference/voice/playai.mdx)
- [音声の例](../../examples/voice/text-to-speech.mdx)
