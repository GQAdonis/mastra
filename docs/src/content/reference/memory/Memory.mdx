# Memory Class Reference

The `Memory` class provides a robust system for managing conversation history and thread-based message storage in Mastra. It enables persistent storage of conversations, semantic search capabilities, and efficient message retrieval. By default, it uses LibSQL for storage and vector search, and FastEmbed for embeddings.

## Basic Usage

```typescript copy showLineNumbers
import { Memory } from "@mastra/memory/vnext";
import { Agent } from "@mastra/core/agent";

const agent = new Agent({
  memory: new Memory(),
  ...otherOptions,
});
```

## Improved Memory Defaults

Based on extensive user feedback and testing, we've created an improved configuration for Memory with more efficient defaults. This implementation is available now via the `/vnext` import path and will become the standard in a future release.

```typescript copy showLineNumbers
import { Memory } from "@mastra/memory/vnext";
```

The key improvements in the defaults:

- **Smaller context window**: Reduced `lastMessages` from 40 to 10, providing a better balance between context and token usage
- **Disabled semantic recall by default**: Many users weren't aware this feature was active and incurring embedding costs
- **Disabled thread title generation**: This removes unnecessary latency when creating new threads  
- **Using tool-calls for working memory**: More reliable than text-stream mode, especially with structured data streams

You can still enable any of these features through configuration when needed.

## Custom Configuration

```typescript copy showLineNumbers
import { Memory } from "@mastra/memory/vnext";
import { LibSQLStore } from "@mastra/core/storage/libsql";
import { LibSQLVector } from "@mastra/core/vector/libsql";
import { Agent } from "@mastra/core/agent";

const memory = new Memory({
  // Optional storage configuration - libsql will be used by default
  storage: new LibSQLStore({
    url: "file:memory.db",
  }),

  // Optional vector database for semantic search - libsql will be used by default
  vector: new LibSQLVector({
    url: "file:vector.db",
  }),

  // Memory configuration options
  options: {
    // Number of recent messages to include
    lastMessages: 20,

    // Semantic search configuration
    semanticRecall: true,
    
    // Title generation for threads
    threads: {
      generateTitle: true,
    },

    // Working memory configuration
    workingMemory: {
      enabled: true,
      template: `
# User
- First Name:
- Last Name:
`,
    },
  },
});

const agent = new Agent({
  memory,
  ...otherOptions,
});
```

### Working Memory

The working memory feature allows agents to maintain persistent information across conversations. When enabled, the Memory class will automatically manage working memory updates through either text stream tags or tool calls.

Example configuration:

```typescript copy showLineNumbers
import { Memory } from "@mastra/memory/vnext";

const memory = new Memory({
  options: {
    workingMemory: {
      enabled: true,
      template: "# User\n- **First Name**:\n- **Last Name**:",
    },
  },
});
```

If no template is provided, the Memory class uses a default template that includes fields for user details, preferences, goals, and other contextual information in Markdown format. See the [Working Memory guide](/docs/memory/working-memory.mdx#designing-effective-templates) for detailed usage examples and best practices.

### embedder

By default, Memory uses FastEmbed with the `bge-small-en-v1.5` model, which provides a good balance of performance and model size (~130MB). You only need to specify an embedder if you want to use a different model or provider.

For environments where local embedding isn't supported, you can use an API-based embedder:

```typescript {6}
import { Memory } from "@mastra/memory/vnext";
import { openai } from "@ai-sdk/openai";

const agent = new Agent({
  memory: new Memory({
    embedder: openai.embedding("text-embedding-3-small"),
  }),
});
```

Mastra supports many embedding models through the [Vercel AI SDK](https://sdk.vercel.ai/docs/ai-sdk-core/embeddings), including options from OpenAI, Google, Mistral, and Cohere.

## Parameters

<PropertiesTable
  content={[
    {
      name: "storage",
      type: "MastraStorage",
      description: "Storage implementation for persisting memory data",
      isOptional: true,
    },
    {
      name: "vector",
      type: "MastraVector",
      description: "Vector store for semantic search capabilities",
      isOptional: true,
    },
    {
      name: "embedder",
      type: "EmbeddingModel",
      description:
        "Embedder instance for vector embeddings. Uses FastEmbed (bge-small-en-v1.5) by default",
      isOptional: true,
    },
    {
      name: "options",
      type: "MemoryConfig",
      description: "General memory configuration options",
      isOptional: true,
    },
  ]}
/>

### options

<PropertiesTable
  content={[
    {
      name: "lastMessages",
      type: "number | false",
      description:
        "Number of most recent messages to retrieve. Set to false to disable.",
      isOptional: true,
      defaultValue: "40 (10 in vNext)",
    },
    {
      name: "semanticRecall",
      type: "boolean | SemanticRecallConfig",
      description:
        "Enable semantic search in message history. Automatically enabled when vector store is provided.",
      isOptional: true,
      defaultValue: "true (false in vNext)",
    },
    {
      name: "topK",
      type: "number",
      description:
        "Number of similar messages to retrieve when using semantic search",
      isOptional: true,
      defaultValue: "2",
    },
    {
      name: "messageRange",
      type: "number | { before: number; after: number }",
      description:
        "Range of messages to include around semantic search results",
      isOptional: true,
      defaultValue: "2",
    },
    {
      name: "workingMemory",
      type: "{ enabled: boolean; template?: string; use?: 'text-stream' | 'tool-call' }",
      description:
        "Configuration for working memory feature that allows persistent storage of user information across conversations. The 'use' setting determines how working memory updates are handled - either through text stream tags or tool calls. Working memory uses Markdown format to structure and store continuously relevant information.",
      isOptional: true,
      defaultValue:
        "{ enabled: false, template: '# User Information\\n- **First Name**:\\n- **Last Name**:\\n...', use: 'text-stream' (use: 'tool-call' in vNext) }",
    },
    {
      name: "threads",
      type: "{ generateTitle?: boolean }",
      description:
        "Settings related to memory thread creation. `generateTitle` will cause the thread.title to be generated from an llm summary of the users first message.",
      isOptional: true,
      defaultValue: "{ generateTitle: true } (false in vNext)",
    },
  ]}
/>

### Related

- [Getting Started with Memory](/docs/memory/getting-started.mdx)
- [Semantic Recall](/docs/memory/semantic-recall.mdx)
- [Working Memory](/docs/memory/working-memory.mdx)
- [Memory Processors](/docs/memory/memory-processors.mdx)
- [createThread](/docs/reference/memory/createThread.mdx)
- [query](/docs/reference/memory/query.mdx)
- [getThreadById](/docs/reference/memory/getThreadById.mdx)
- [getThreadsByResourceId](/docs/reference/memory/getThreadsByResourceId.mdx)

