---
title: "例: 幻覚 | Evals | Mastra Docs"
description: 応答における事実の矛盾を評価するために幻覚メトリックを使用する例。
---

import { GithubLink } from "../../../../components/github-link";

# Hallucination

この例では、MastraのHallucinationメトリックを使用して、応答がコンテキストで提供された情報と矛盾しているかどうかを評価する方法を示します。

## 概要

この例では、以下の方法を示します：

1. Hallucinationメトリックを設定する
2. 事実の矛盾を評価する
3. 幻覚スコアを分析する
4. 異なる精度レベルを処理する

## セットアップ

### 環境セットアップ

環境変数を設定してください：

```bash filename=".env"
OPENAI_API_KEY=your_api_key_here
```

### 依存関係

必要な依存関係をインポートします：

```typescript copy showLineNumbers filename="src/index.ts"
import { openai } from '@ai-sdk/openai';
import { HallucinationMetric } from '@mastra/evals/llm';
```

## 使用例

### ハルシネーションなしの例

コンテキストに正確に一致する応答を評価します：

```typescript copy showLineNumbers{5} filename="src/index.ts"
const context1 = [
  'iPhoneは2007年に初めて発売されました。',
  'スティーブ・ジョブズがMacworldで発表しました。',
  'オリジナルモデルは3.5インチの画面を持っていました。',
];

const metric1 = new HallucinationMetric(openai('gpt-4o-mini'), {
  context: context1,
});

const query1 = '最初のiPhoneはいつ発売されましたか？';
const response1 = 'iPhoneは2007年に初めて発売され、スティーブ・ジョブズがMacworldで発表しました。オリジナルのiPhoneは3.5インチの画面を備えていました。';

console.log('例1 - ハルシネーションなし:');
console.log('コンテキスト:', context1);
console.log('クエリ:', query1);
console.log('応答:', response1);

const result1 = await metric1.measure(query1, response1);
console.log('メトリック結果:', {
  score: result1.score,
  reason: result1.info.reason,
});
// 出力例:
// メトリック結果: { score: 0, reason: '応答はコンテキストに正確に一致します。' }
```

### 混合ハルシネーションの例

いくつかの事実に矛盾する応答を評価します：

```typescript copy showLineNumbers{31} filename="src/index.ts"
const context2 = [
  '最初のスター・ウォーズ映画は1977年に公開されました。',
  'ジョージ・ルーカスが監督しました。',
  '映画は全世界で7億7500万ドルを稼ぎました。',
  '映画はチュニジアとイギリスで撮影されました。',
];

const metric2 = new HallucinationMetric(openai('gpt-4o-mini'), {
  context: context2,
});

const query2 = '最初のスター・ウォーズ映画について教えてください。';
const response2 = '最初のスター・ウォーズ映画は1977年に公開され、ジョージ・ルーカスが監督しました。興行収入は10億ドルを超え、全てカリフォルニアで撮影されました。';

console.log('例2 - 混合ハルシネーション:');
console.log('コンテキスト:', context2);
console.log('クエリ:', query2);
console.log('応答:', response2);

const result2 = await metric2.measure(query2, response2);
console.log('メトリック結果:', {
  score: result2.score,
  reason: result2.info.reason,
});
// 出力例:
// メトリック結果: { score: 0.5, reason: '応答はいくつかの事実に矛盾しています。' }
```

### 完全なハルシネーションの例

すべての事実に矛盾する応答を評価します：

```typescript copy showLineNumbers{58} filename="src/index.ts"
const context3 = [
  'ライト兄弟は1903年に初飛行を行いました。',
  '飛行は12秒間続きました。',
  '120フィートの距離をカバーしました。',
];

const metric3 = new HallucinationMetric(openai('gpt-4o-mini'), {
  context: context3,
});

const query3 = 'ライト兄弟はいつ初飛行をしましたか？';
const response3 = 'ライト兄弟は1908年に歴史的な初飛行を達成しました。飛行は約2分間続き、ほぼ1マイルをカバーしました。';

console.log('例3 - 完全なハルシネーション:');
console.log('コンテキスト:', context3);
console.log('クエリ:', query3);
console.log('応答:', response3);

const result3 = await metric3.measure(query3, response3);
console.log('メトリック結果:', {
  score: result3.score,
  reason: result3.info.reason,
});
// 出力例:
// メトリック結果: { score: 1, reason: '応答はコンテキストに完全に矛盾しています。' }
```

## 結果の理解

この指標は以下を提供します：

1. 0から1の間の幻覚スコア：
   - 0.0: 幻覚なし - 文脈との矛盾なし
   - 0.3-0.4: 低い幻覚 - 矛盾が少ない
   - 0.5-0.6: 混合幻覚 - いくつかの矛盾
   - 0.7-0.8: 高い幻覚 - 多くの矛盾
   - 0.9-1.0: 完全な幻覚 - すべての文脈と矛盾

2. スコアの詳細な理由、以下を含む分析：
   - 発言の検証
   - 見つかった矛盾
   - 事実の正確性
   - 全体的な幻覚レベル

<br />
<br />
<hr className="dark:border-[#404040] border-gray-300" />
<br />
<br />
<GithubLink
  link={
    "https://github.com/mastra-ai/mastra/blob/main/examples/basics/evals/hallucination"
  }
/>
