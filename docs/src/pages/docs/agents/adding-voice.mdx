---
title: Adding Voice to Agents | Mastra Docs
description: A comprehensive guide to integrating voice capabilities with Mastra agents, enabling conversational interfaces through speech.
---

import { Callout } from "nextra/components";


# Adding Voice to Agents

<Callout>
  Integrating voice capabilities with Mastra agents enables natural spoken interactions, transforming text-based systems into conversational assistants.
</Callout>

## Overview

Mastra's agent architecture seamlessly integrates with voice capabilities, allowing your AI systems to listen to users, process their speech, and respond with natural-sounding voice. This integration supports both simple speak/listen patterns and continuous real-time conversations.

## Basic Integration

### Setting Up a Voice-Enabled Agent

The simplest way to add voice to an agent is by providing a voice provider during initialization:

```typescript
import { Agent } from "@mastra/core/agent";
import { OpenAIVoice } from "@mastra/voice-openai";

// Create a voice provider
const voice = new OpenAIVoice({
  speechModel: {
    name: 'tts-1-hd',
    apiKey: process.env.OPENAI_API_KEY,
  },
  listeningModel: {
    name: 'whisper-1',
    apiKey: process.env.OPENAI_API_KEY,
  },
  speaker: 'alloy',
});

// Create an agent with voice capabilities
const agent = new Agent({
  name: 'CustomerSupport',
  instructions: 'You are a helpful customer support assistant.',
  model: openai('gpt-4o'),
  voice: voice,
});

// Now the agent can speak and listen
```

### Basic Voice Interactions

Once your agent has voice capabilities, you can use it for speech synthesis and recognition:

```typescript
// Convert agent responses to speech
const response = await agent.generate("How do I reset my password?");
const audioStream = await agent.voice.speak(response.text);
playAudio(audioStream); // Your implementation

// Process user speech
const audioInput = getUserAudioStream(); // Your implementation
const transcript = await agent.voice.listen(audioInput);
console.log("User said:", transcript);

// Generate a response based on the transcript
const nextResponse = await agent.generate(transcript);
```

## Conversation Patterns

### Request-Response Pattern

This pattern handles voice interactions as discrete turns:

```typescript
async function handleConversationTurn(audioInput) {
  // 1. Listen to user input
  const transcript = await agent.voice.listen(audioInput);
  
  // 2. Generate a response
  const response = await agent.generate(transcript);
  
  // 3. Convert response to speech
  const audioOutput = await agent.voice.speak(response.text);
  
  // 4. Return both text and audio
  return {
    userText: transcript,
    agentText: response.text,
    agentAudio: audioOutput,
  };
}
```

## Real-Time Voice Conversations

For more natural interactions, you can use real-time voice capabilities:

```typescript
import { Agent } from "@mastra/core/agent";
import { OpenAIRealtimeVoice } from "@mastra/voice-openai-realtime";

// Initialize real-time voice provider
const realtimeVoice = new OpenAIRealtimeVoice({
  chatModel: {
    apiKey: process.env.OPENAI_API_KEY,
    model: 'gpt-4o-mini-realtime',
  },
  speaker: 'alloy',
});

// Create agent with real-time voice
const agent = new Agent({
  name: 'VoiceAssistant',
  instructions: 'You are a helpful voice assistant.',
  model: openai('gpt-4o'),
  voice: realtimeVoice,
});

// Set up a real-time conversation
async function startRealtimeConversation() {
  // 1. Set up event listeners
  agent.voice.on('speaking', ({ audio }) => {
    playAudioChunk(audio); // Play audio chunks as they arrive
  });
  
  agent.voice.on('writing', ({ text, role }) => {
    updateTranscript(role, text); // Update UI with transcription
  });
  
  // 2. Connect to the service
  await agent.voice.connect();
  
  // 3. Start with a greeting
  await agent.voice.speak('Hello, how can I help you today?');
  
  // 4. Start listening for user input
  const micStream = getMicrophoneStream(); // Your implementation
  await agent.voice.send(micStream);
  
  // 5. Voice activity detection will automatically handle turn-taking
}
```

## Using Multiple Voice Providers with Agents

You can combine different providers to leverage their strengths:

```typescript
import { CompositeVoice } from "@mastra/core/voice";
import { OpenAIVoice } from "@mastra/voice-openai";
import { ElevenLabsVoice } from "@mastra/voice-elevenlabs";

// Create providers
const sttProvider = new OpenAIVoice({
  listeningModel: {
    name: 'whisper-1',
    apiKey: process.env.OPENAI_API_KEY,
  },
});

const ttsProvider = new ElevenLabsVoice({
  speechModel: {
    name: 'eleven_multilingual_v2',
    apiKey: process.env.ELEVENLABS_API_KEY,
  },
  speaker: '9BWtsMINqrJLrRacOk9x', // Aria voice
});

// Combine providers
const compositeVoice = new CompositeVoice({
  listeningProvider: sttProvider,
  speakingProvider: ttsProvider,
});

// Create agent with composite voice
const agent = new Agent({
  name: 'MultiProviderAgent',
  instructions: 'You are a helpful assistant with a premium voice.',
  model: openai('gpt-4o'),
  voice: compositeVoice,
});
```

## Advanced Use Cases

### Voice with Memory

Combining voice with memory for context-aware conversations:

```typescript
import { Agent } from "@mastra/core/agent";
import { OpenAIVoice } from "@mastra/voice-openai";
import { Memory } from "@mastra/core/memory";

// Initialize memory
const memory = new Memory();

// Create voice-enabled agent with memory
const agent = new Agent({
  name: 'MemoryVoiceAgent',
  instructions: 'You are a helpful assistant that remembers conversations.',
  model: openai('gpt-4o'),
  voice: new OpenAIVoice(),
  memory: memory,
});

// Run a voice conversation with memory
async function voiceConversationWithMemory(threadId, resourceId) {
  // Get user audio input
  const userAudio = await recordUserAudio();
  
  // Convert to text
  const userText = await agent.voice.listen(userAudio);
  
  // Generate response with memory context
  const response = await agent.generate(userText, {
    threadId,
    resourceId,
    memoryOptions: {
      vectors: { enabled: true },
    },
  });
  
  // Speak response
  const audioResponse = await agent.voice.speak(response.text);
  playAudio(audioResponse);
  
  return response;
}
```

### Voice with Tools

Enabling voice-driven tool use:

```typescript
import { Agent } from "@mastra/core/agent";
import { OpenAIVoice } from "@mastra/voice-openai";
import { createTool } from "@mastra/core/tools";
import { z } from "zod";

// Create a weather tool
const weatherTool = createTool({
  id: "GetWeather",
  description: "Get the current weather for a location",
  inputSchema: z.object({
    location: z.string().describe("The city or location to get weather for"),
  }),
  execute: async ({ location }) => {
    // Implementation to fetch weather
    return { temperature: 72, conditions: "Sunny", location };
  },
});

// Create voice-enabled agent with tools
const agent = new Agent({
  name: 'VoiceToolAgent',
  instructions: 'You are a helpful assistant that can check the weather.',
  model: openai('gpt-4o'),
  voice: new OpenAIVoice(),
  tools: { weatherTool },
});

// Voice conversation with tool use
async function checkWeatherByVoice() {
  // Prompt user
  await agent.voice.speak("Which city's weather would you like to know?");
  
  // Get location from voice
  const userAudio = await recordUserAudio();
  const userResponse = await agent.voice.listen(userAudio);
  
  // Generate response (will use tool automatically if needed)
  const response = await agent.generate(userResponse, {
    temperature: 0,
    toolChoice: "auto",
  });
  
  // Speak the response
  const audioResponse = await agent.voice.speak(response.text);
  playAudio(audioResponse);
}
```

## Best Practices

### Voice-Optimized Prompting

Craft your agent instructions for voice interaction:

```typescript
const agent = new Agent({
  name: 'VoiceOptimizedAgent',
  instructions: `You are a helpful voice assistant. 
  - Keep your responses concise and conversational.
  - Use simple language that's easy to understand when heard rather than read.
  - Break complex information into smaller chunks.
  - When listing options, limit to 3-4 choices at a time.
  - Use verbal cues like "first," "next," and "finally" to guide the conversation.
  - Confirm understanding when appropriate.
  - Ask clarifying questions when needed.`,
  model: openai('gpt-4o'),
  voice: new OpenAIVoice(),
});
```

### Error Handling for Voice Interactions

Implement robust error handling for voice:

```typescript
async function robustVoiceInteraction(prompt) {
  try {
    // Attempt to generate and speak response
    const response = await agent.generate(prompt);
    const audio = await agent.voice.speak(response.text);
    return audio;
  } catch (error) {
    console.error("Voice error:", error);
    
    // Handle different error types
    if (error.message.includes('API key')) {
      // Authentication error
      return agent.voice.speak("I'm having trouble connecting. Please try again later.");
    } else if (error.message.includes('audio')) {
      // Audio processing error
      return agent.voice.speak("I had trouble processing that. Could you repeat it?");
    } else {
      // Generic error
      return agent.voice.speak("I apologize, but I encountered an unexpected issue.");
    }
  }
}
```

### Voice UX Considerations

- **Confirm actions**: "I'll send that email for you. Should I proceed?"
- **Provide feedback**: "I'm looking that up now..." during tool execution
- **Handle interruptions**: Allow users to interrupt long responses
- **Give turn-taking cues**: Subtle indicators when you're ready for user input
- **Adapt to user speech patterns**: Match pace and formality level

## Troubleshooting

### Agent Not Responding to Voice

- Check if the voice provider is properly initialized
- Verify API keys are valid
- Ensure audio input is being captured correctly
- Test voice provider separately from agent
- Check for conflicts with other audio devices

### Poor Transcription Quality

- Improve microphone quality/placement
- Reduce background noise
- Try a different STT provider
- For specialized terminology, use a provider with custom vocabulary

### Voice Not Working with Tools

- Ensure tools are properly configured
- Check that tool schema matches expected inputs
- Use more explicit prompting for tool invocation
- Adjust toolChoice parameter to "auto" or specific tool

## Next Steps

- Learn about [Voice-to-Voice](../voice/voice-to-voice) for real-time capabilities
- Explore [Voice Providers](../reference/voice/providers) to compare available options
- Check out [Common Voice Use Cases](../voice/use-cases/common) for more implementation patterns
- Dive into [Voice UI Design](../voice/use-cases/voice-ui-design) for best practices
