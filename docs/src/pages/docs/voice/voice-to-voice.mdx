---
title: Voice-to-Voice in Mastra | Mastra Docs
description: A comprehensive guide to implementing real-time voice conversations in Mastra, from basic setup to advanced features and best practices.
---

import { Callout } from "nextra/components";

# Voice-to-Voice

<Callout>
  Voice-to-Voice enables real-time speech-to-speech interactions, creating natural conversational experiences between users and AI agents.
</Callout>

## Introduction

Voice-to-Voice in Mastra provides a standardized interface for real-time speech-to-speech interactions across multiple service providers. Unlike separate TTS and STT operations, Voice-to-Voice maintains a continuous connection for fluid conversations, similar to speaking with another person.

## Key Concepts

### How Voice-to-Voice Differs from Separate TTS/STT

| Feature | Separate TTS/STT | Voice-to-Voice |
| --- | --- | --- |
| **Connection** | One-off requests | Continuous WebSocket connection |
| **Interaction Pattern** | Request-response | Stream-based conversation |
| **Turn Management** | Manual | Automatic with voice activity detection |
| **Latency** | Higher | Lower with streaming responses |
| **Implementation** | Simpler | More complex but more natural |

### Event-Driven Architecture

Voice-to-Voice uses an event-driven architecture to handle real-time communication:

- **Events from User**: Audio streaming, silence detection, interruptions
- **Events from System**: Speaking, thinking, processing, tool usage
- **Event Handlers**: Custom logic for responding to various states

This approach allows for more responsive interactions than waiting for complete audio responses.

## Quick Start

Here's how to set up a basic real-time voice conversation using OpenAI:

```typescript
import { Agent } from "@mastra/core/agent";
import { OpenAIRealtimeVoice } from "@mastra/voice-openai-realtime";

// Step 1: Create a voice provider with real-time capabilities
const voice = new OpenAIRealtimeVoice({
  chatModel: {
    apiKey: process.env.OPENAI_API_KEY,
    model: 'gpt-4o-mini-realtime',
  },
  speaker: 'alloy',
});

// Step 2: Create an agent with the voice provider
const agent = new Agent({
  name: 'VoiceAssistant',
  instructions: 'You are a helpful voice assistant that responds concisely.',
  model: openai('gpt-4o'),
  voice: voice,
});

// Step 3: Set up event listeners
agent.voice.on('speaking', ({ audio }) => {
  // Handle audio data as it arrives
  playAudio(audio);
});

agent.voice.on('writing', ({ text, role }) => {
  // Show transcription as it happens
  console.log(`${role}: ${text}`);
});

// Step 4: Connect to the service
await agent.voice.connect();

// Step 5: Start the conversation
await agent.voice.speak('Hello, how can I help you today?');

// Step 6: Send user audio
const micStream = getMicrophoneStream(); // Your implementation
await agent.voice.send(micStream);

// Step 7: When done, close the connection
voice.close();
```

## Configuration

When setting up real-time voice, you can customize various aspects of the experience:

```typescript
const voice = new OpenAIRealtimeVoice({
  chatModel: {
    apiKey: process.env.OPENAI_API_KEY,
    model: 'gpt-4o-mini-realtime',
    options: {
      sessionConfig: {
        // Configure turn detection
        turn_detection: {
          type: 'server_vad',         // Server-side voice activity detection
          threshold: 0.6,             // Speech detection sensitivity (0.0-1.0)
          prefix_padding_ms: 1000,    // Include audio before speech starts
          silence_duration_ms: 1200,  // Time of silence before ending turn
        },
      },
    },
  },
  speaker: 'echo',  // Default voice
});
```

### Voice Activity Detection (VAD)

VAD is critical for real-time conversation, as it determines when a user has stopped speaking and the AI should respond:

| Parameter | Description | Recommended Values |
| --- | --- | --- |
| `type` | VAD implementation to use | `'server_vad'` for better accuracy |
| `threshold` | Detection sensitivity | `0.5-0.7` (higher = more sensitive) |
| `silence_duration_ms` | Silence before turn ends | `800-1500ms` (user-dependent) |
| `prefix_padding_ms` | Audio to include before speech | `500-1000ms` (context-dependent) |

## The Event System

Real-time voice leverages events to handle asynchronous communication:

```typescript
// Speaking events (when AI speaks)
voice.on('speaking', ({ audio }) => {
  // Audio is a stream chunk in the format specified
  playAudioChunk(audio);
});

// Writing events (transcription events)
voice.on('writing', ({ text, role, done }) => {
  console.log(`${role}: ${text}`);
  if (done) {
    console.log("Transcription complete!");
  }
});

// Error events
voice.on('error', (error) => {
  console.error("Voice error:", error);
});

// You can also listen to OpenAI-specific events
voice.on('openAIRealtime:conversation.interrupted', () => {
  console.log("Conversation was interrupted");
});
```

## Handling Continuous Audio

Managing continuous audio streams is different from one-off TTS/STT:

```typescript
// Get microphone stream (implementation depends on environment)
function getMicrophoneStream() {
  // Browser implementation
  return navigator.mediaDevices.getUserMedia({ audio: true })
    .then(stream => {
      const audioContext = new AudioContext();
      const source = audioContext.createMediaStreamSource(stream);
      
      // Process for required format (usually 16kHz mono PCM)
      const processor = audioContext.createScriptProcessor(4096, 1, 1);
      source.connect(processor);
      processor.connect(audioContext.destination);
      
      // Return as appropriate stream format
      return createAudioStream(processor);
    });
}

// Send continuous audio
const micStream = await getMicrophoneStream();
await voice.send(micStream);

// Stop audio when needed
micStream.unpipe();
```

## Advanced Topics

### Adding Tools to Voice Conversations

Voice agents can use tools during conversations, just like text-based agents:

```typescript
import { createTool } from '@mastra/core/tools';
import { z } from 'zod';

// Define a weather tool
const weatherTool = createTool({
  id: "Get Weather Information",
  inputSchema: z.object({
    city: z.string(),
  }),
  description: "Fetches the current weather information for a given city",
  execute: async ({ city }) => {
    // Implementation to fetch weather data
    return { temperature: 72, conditions: "Sunny", city };
  },
});

// Add to voice provider
voice.addTools([weatherTool]);

// Now conversations can use weather information
// "What's the weather in San Francisco?" will trigger the tool
```

### Handling Conversation Flow

You can implement custom conversation management:

```typescript
// Listen for specific events to manage the conversation
let conversationState = 'initial';

voice.on('writing', ({ text, role }) => {
  if (role === 'user') {
    // Process user input
    if (text.toLowerCase().includes('goodbye')) {
      conversationState = 'ending';
    }
  }
});

voice.on('speaking', ({ }) => {
  // If we're in ending state, close after response
  if (conversationState === 'ending') {
    setTimeout(() => {
      voice.speak("It was nice talking to you. Goodbye!");
      setTimeout(() => voice.close(), 3000);
    }, 1000);
  }
});
```

### Multi-Turn Conversations

Managing context across multiple turns:

```typescript
// Set up tracking for conversation state
let conversationContext = {
  topic: null,
  userPreferences: {},
  interactionCount: 0,
};

// Update context based on transcription
voice.on('writing', ({ text, role }) => {
  if (role === 'user') {
    conversationContext.interactionCount++;
    
    // Simple topic detection
    if (text.includes('weather')) conversationContext.topic = 'weather';
    else if (text.includes('news')) conversationContext.topic = 'news';
    
    // Track user preferences
    if (text.includes('prefer')) {
      // Extract preference
    }
  }
});

// Use context in responses
voice.on('speaking', () => {
  if (conversationContext.interactionCount > 5) {
    // Maybe suggest a more focused topic after several exchanges
  }
});
```

## Best Practices

### Optimizing Voice Interactions

- **Keep AI responses concise** - Brief responses feel more natural in voice conversations
- **Use context-dependent delays** - Add small pauses between turns for natural rhythm
- **Handle interruptions gracefully** - Allow users to interrupt long responses
- **Provide audio feedback** - Use short sounds to indicate listening/processing
- **Consider environmental factors** - Background noise can affect VAD performance

### Performance Tuning

- **Adjust VAD parameters** based on your users' speaking patterns
- **Optimize audio processing** to reduce latency
- **Use appropriate sampling rates** (16kHz is typically sufficient)
- **Select voices based on clarity** rather than just personality
- **Monitor and log conversation metrics** to identify improvement areas

### Error Handling

Always implement robust error handling:

```typescript
// Connection errors
try {
  await voice.connect();
} catch (error) {
  console.error("Connection failed:", error);
  // Implement fallback to text-only mode
}

// Runtime errors
voice.on('error', (error) => {
  if (error.code === 'NETWORK_ERROR') {
    // Handle network issues
    attemptReconnect();
  } else if (error.code === 'AUDIO_PROCESSING_ERROR') {
    // Handle audio issues
    switchToBackupAudioPipeline();
  } else {
    // General error handling
    logError(error);
    notifyUser("I'm having trouble understanding. Could you try again?");
  }
});
```

## Troubleshooting

### Poor Voice Detection

- Adjust VAD threshold (increase for sensitivity, decrease for stability)
- Increase silence_duration_ms if the system interrupts too early
- Check for background noise affecting detection
- Try different microphone placement or quality

### High Latency

- Ensure good network connectivity
- Reduce audio quality if bandwidth is limited
- Consider using a more lightweight model
- Implement client-side pre-processing to reduce server load

### Connection Issues

- Verify API keys and permissions
- Check network firewall settings (WebSockets may be blocked)
- Implement reconnection logic with exponential backoff
- Monitor connection health with periodic pings

## Next Steps

- Explore [OpenAI Realtime](../reference/voice/openai-realtime) for detailed API reference
- Learn about [Adding Voice to Agents](../agents/adding-voice) for deeper integration
- Try [Voice with Tools](./use-cases/voice-with-tools) for enhanced capabilities
- See [Voice User Interface Design](./use-cases/voice-ui-design) for best practices